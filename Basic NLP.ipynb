{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8675, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw|||...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>'I'm finding the lack of me in these posts ver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INTP</td>\n",
       "      <td>'Good one  _____   https://www.youtube.com/wat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>'Dear INTP,   I enjoyed our conversation the o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTJ</td>\n",
       "      <td>'You're fired.|||That's another silly misconce...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                              posts\n",
       "0  INFJ  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...\n",
       "1  ENTP  'I'm finding the lack of me in these posts ver...\n",
       "2  INTP  'Good one  _____   https://www.youtube.com/wat...\n",
       "3  INTJ  'Dear INTP,   I enjoyed our conversation the o...\n",
       "4  ENTJ  'You're fired.|||That's another silly misconce..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"./data/mbti-type.zip\", compression=\"zip\")\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/chris/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "['test', 'text']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk import download as nltk_download\n",
    "import string\n",
    "\n",
    "nltk_download(\"stopwords\")\n",
    "\n",
    "tokenizer = TweetTokenizer() # using TweetTokenizer even though these aren't tweets per se\n",
    "stopwords = set(nltk_stopwords.words('english'))\n",
    "\n",
    "punc_stripper = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "def preprocess(text):\n",
    "    # lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # strip punctuation\n",
    "    text = text.translate(punc_stripper)\n",
    "    \n",
    "    # tokenize\n",
    "    tokenized = tokenizer.tokenize(text)\n",
    "    \n",
    "    # remove stopwords\n",
    "    unstopped = [t for t in tokenized if t not in stopwords]\n",
    "    \n",
    "    return unstopped\n",
    "\n",
    "test_text = \"This is Some test text!\"\n",
    "print(preprocess(test_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['preprocessed'] = df.posts.apply(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-Gram analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['my name is', 'name is Carl']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.util import ngrams as nltk_ngrams\n",
    "\n",
    "def ngram_extract(tokens, n):\n",
    "    ngrams = list(nltk_ngrams(tokens, n))\n",
    "    ngram_phrases = [\" \".join(x) for x in ngrams]\n",
    "    return ngram_phrases\n",
    "    \n",
    "test_ngrams = [\"my\", \"name\", \"is\", \"Carl\"]\n",
    "ngram_extract(test_ngrams, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "      <th>preprocessed</th>\n",
       "      <th>unigrams</th>\n",
       "      <th>bigrams</th>\n",
       "      <th>trigrams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw|||...</td>\n",
       "      <td>[httpwwwyoutubecomwatchvqsxhcwe, 3krwhttp41med...</td>\n",
       "      <td>[httpwwwyoutubecomwatchvqsxhcwe, 3krwhttp41med...</td>\n",
       "      <td>[httpwwwyoutubecomwatchvqsxhcwe 3krwhttp41medi...</td>\n",
       "      <td>[httpwwwyoutubecomwatchvqsxhcwe 3krwhttp41medi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>'I'm finding the lack of me in these posts ver...</td>\n",
       "      <td>[im, finding, lack, posts, alarmingsex, boring...</td>\n",
       "      <td>[im, finding, lack, posts, alarmingsex, boring...</td>\n",
       "      <td>[im finding, finding lack, lack posts, posts a...</td>\n",
       "      <td>[im finding lack, finding lack posts, lack pos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INTP</td>\n",
       "      <td>'Good one  _____   https://www.youtube.com/wat...</td>\n",
       "      <td>[good, one, httpswwwyoutubecomwatchvfhigbolffg...</td>\n",
       "      <td>[good, one, httpswwwyoutubecomwatchvfhigbolffg...</td>\n",
       "      <td>[good one, one httpswwwyoutubecomwatchvfhigbol...</td>\n",
       "      <td>[good one httpswwwyoutubecomwatchvfhigbolffgwo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>'Dear INTP,   I enjoyed our conversation the o...</td>\n",
       "      <td>[dear, intp, enjoyed, conversation, day, esote...</td>\n",
       "      <td>[dear, intp, enjoyed, conversation, day, esote...</td>\n",
       "      <td>[dear intp, intp enjoyed, enjoyed conversation...</td>\n",
       "      <td>[dear intp enjoyed, intp enjoyed conversation,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTJ</td>\n",
       "      <td>'You're fired.|||That's another silly misconce...</td>\n",
       "      <td>[youre, firedthats, another, silly, misconcept...</td>\n",
       "      <td>[youre, firedthats, another, silly, misconcept...</td>\n",
       "      <td>[youre firedthats, firedthats another, another...</td>\n",
       "      <td>[youre firedthats another, firedthats another ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                              posts  \\\n",
       "0  INFJ  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...   \n",
       "1  ENTP  'I'm finding the lack of me in these posts ver...   \n",
       "2  INTP  'Good one  _____   https://www.youtube.com/wat...   \n",
       "3  INTJ  'Dear INTP,   I enjoyed our conversation the o...   \n",
       "4  ENTJ  'You're fired.|||That's another silly misconce...   \n",
       "\n",
       "                                        preprocessed  \\\n",
       "0  [httpwwwyoutubecomwatchvqsxhcwe, 3krwhttp41med...   \n",
       "1  [im, finding, lack, posts, alarmingsex, boring...   \n",
       "2  [good, one, httpswwwyoutubecomwatchvfhigbolffg...   \n",
       "3  [dear, intp, enjoyed, conversation, day, esote...   \n",
       "4  [youre, firedthats, another, silly, misconcept...   \n",
       "\n",
       "                                            unigrams  \\\n",
       "0  [httpwwwyoutubecomwatchvqsxhcwe, 3krwhttp41med...   \n",
       "1  [im, finding, lack, posts, alarmingsex, boring...   \n",
       "2  [good, one, httpswwwyoutubecomwatchvfhigbolffg...   \n",
       "3  [dear, intp, enjoyed, conversation, day, esote...   \n",
       "4  [youre, firedthats, another, silly, misconcept...   \n",
       "\n",
       "                                             bigrams  \\\n",
       "0  [httpwwwyoutubecomwatchvqsxhcwe 3krwhttp41medi...   \n",
       "1  [im finding, finding lack, lack posts, posts a...   \n",
       "2  [good one, one httpswwwyoutubecomwatchvfhigbol...   \n",
       "3  [dear intp, intp enjoyed, enjoyed conversation...   \n",
       "4  [youre firedthats, firedthats another, another...   \n",
       "\n",
       "                                            trigrams  \n",
       "0  [httpwwwyoutubecomwatchvqsxhcwe 3krwhttp41medi...  \n",
       "1  [im finding lack, finding lack posts, lack pos...  \n",
       "2  [good one httpswwwyoutubecomwatchvfhigbolffgwo...  \n",
       "3  [dear intp enjoyed, intp enjoyed conversation,...  \n",
       "4  [youre firedthats another, firedthats another ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    \n",
    "df['unigrams'], df['bigrams'], df['trigrams'] = [df.preprocessed.apply(lambda x: ngram_extract(x, n + 1)) for n in range(3)]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>unigrams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>httpwwwyoutubecomwatchvqsxhcwe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>3krwhttp41mediatumblrcomtumblrlfouy03pma1qa1ro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>intj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>moments</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>httpswwwyoutubecomwatchviz</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                           unigrams\n",
       "0  INFJ                     httpwwwyoutubecomwatchvqsxhcwe\n",
       "1  INFJ  3krwhttp41mediatumblrcomtumblrlfouy03pma1qa1ro...\n",
       "2  INFJ                                               intj\n",
       "3  INFJ                                            moments\n",
       "4  INFJ                         httpswwwyoutubecomwatchviz"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = list()\n",
    "for row in df[['type', 'unigrams']].iterrows():\n",
    "    r = row[1]\n",
    "    for word in r.unigrams:\n",
    "        rows.append((r.type, word))\n",
    "unigrams = pd.DataFrame(rows, columns=[\"type\", \"unigrams\"])\n",
    "unigrams.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A less stupid way to extract n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ENFJ': {1: Counter(), 2: Counter(), 3: Counter()},\n",
      " 'ENFP': {1: Counter(), 2: Counter(), 3: Counter()},\n",
      " 'ENTJ': {1: Counter(), 2: Counter(), 3: Counter()},\n",
      " 'ENTP': {1: Counter(), 2: Counter(), 3: Counter()},\n",
      " 'ESFJ': {1: Counter(), 2: Counter(), 3: Counter()},\n",
      " 'ESFP': {1: Counter(), 2: Counter(), 3: Counter()},\n",
      " 'ESTJ': {1: Counter(), 2: Counter(), 3: Counter()},\n",
      " 'ESTP': {1: Counter(), 2: Counter(), 3: Counter()},\n",
      " 'INFJ': {1: Counter(), 2: Counter(), 3: Counter()},\n",
      " 'INFP': {1: Counter(), 2: Counter(), 3: Counter()},\n",
      " 'INTJ': {1: Counter(), 2: Counter(), 3: Counter()},\n",
      " 'INTP': {1: Counter(), 2: Counter(), 3: Counter()},\n",
      " 'ISFJ': {1: Counter(), 2: Counter(), 3: Counter()},\n",
      " 'ISFP': {1: Counter(), 2: Counter(), 3: Counter()},\n",
      " 'ISTJ': {1: Counter(), 2: Counter(), 3: Counter()},\n",
      " 'ISTP': {1: Counter(), 2: Counter(), 3: Counter()}}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter()\n",
    "\n",
    "types = df['type'].unique()\n",
    "ns = [1,2,3]\n",
    "\n",
    "n_gram_freqs_by_type = {}\n",
    "for type in types:\n",
    "    n_gram_freqs = {}\n",
    "    for n in ns:\n",
    "        n_gram_freqs[n] = Counter()\n",
    "    n_gram_freqs_by_type[type] = n_gram_freqs\n",
    "pp.pprint(n_gram_freqs_by_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf6d8efd6a0044a0bc964733f04a642b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "for row_id, row in tqdm(df[[\"type\", \"preprocessed\"]].iterrows()):\n",
    "    #get ngrams for all values of n defined previously\n",
    "    for n in ns:\n",
    "        grams = ngrams(row.preprocessed, n)\n",
    "        for gram in grams:\n",
    "            n_gram_freqs_by_type[row.type][n][gram] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the extracted n-grams per personality type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFJ\n",
      "\tMost common 1-grams:\n",
      "\t\tim\t11573\n",
      "\t\tlike\t11477\n",
      "\t\tthink\t8594\n",
      "\t\tdont\t8449\n",
      "\t\tpeople\t7950\n",
      "\tMost common 2-grams:\n",
      "\t\tdont know\t1272\n",
      "\t\tfeel like\t1118\n",
      "\t\tdont think\t877\n",
      "\t\tim sure\t540\n",
      "\t\tdont want\t424\n",
      "\tMost common 3-grams:\n",
      "\t\tsent iphone using\t137\n",
      "\t\tfeel like im\t96\n",
      "\t\tim pretty sure\t80\n",
      "\t\tknow youre infj\t71\n",
      "\t\tdont really know\t56\n",
      "ENTP\n",
      "\tMost common 1-grams:\n",
      "\t\tlike\t5133\n",
      "\t\tim\t4857\n",
      "\t\tdont\t3855\n",
      "\t\tthink\t3835\n",
      "\t\tpeople\t3379\n",
      "\tMost common 2-grams:\n",
      "\t\tdont know\t490\n",
      "\t\tdont think\t398\n",
      "\t\tfeel like\t310\n",
      "\t\tim sure\t209\n",
      "\t\tdont really\t191\n",
      "\tMost common 3-grams:\n",
      "\t\tim pretty sure\t69\n",
      "\t\tsent iphone using\t48\n",
      "\t\tknow youre entp\t42\n",
      "\t\tdont feel like\t27\n",
      "\t\tnucky nucky nucky\t25\n",
      "INTP\n",
      "\tMost common 1-grams:\n",
      "\t\tlike\t9325\n",
      "\t\tim\t8698\n",
      "\t\tdont\t7704\n",
      "\t\tthink\t6924\n",
      "\t\tpeople\t6498\n",
      "\tMost common 2-grams:\n",
      "\t\tdont know\t1012\n",
      "\t\tdont think\t730\n",
      "\t\tfeel like\t594\n",
      "\t\tim sure\t409\n",
      "\t\tdont like\t387\n",
      "\tMost common 3-grams:\n",
      "\t\tim pretty sure\t102\n",
      "\t\tfeel like im\t62\n",
      "\t\tdont really know\t60\n",
      "\t\tdont even know\t55\n",
      "\t\tknow youre intp\t51\n",
      "INTJ\n",
      "\tMost common 1-grams:\n",
      "\t\tlike\t7509\n",
      "\t\tim\t6650\n",
      "\t\tdont\t6250\n",
      "\t\tpeople\t5457\n",
      "\t\tthink\t5413\n",
      "\tMost common 2-grams:\n",
      "\t\tdont know\t787\n",
      "\t\tdont think\t635\n",
      "\t\tfeel like\t446\n",
      "\t\tim sure\t345\n",
      "\t\tdont really\t314\n",
      "\tMost common 3-grams:\n",
      "\t\tsent iphone using\t93\n",
      "\t\tim pretty sure\t51\n",
      "\t\tknow youre intj\t46\n",
      "\t\tdont really care\t44\n",
      "\t\tfeel like im\t36\n",
      "ENTJ\n",
      "\tMost common 1-grams:\n",
      "\t\tlike\t1607\n",
      "\t\tim\t1512\n",
      "\t\tdont\t1301\n",
      "\t\tthink\t1257\n",
      "\t\tpeople\t1109\n",
      "\tMost common 2-grams:\n",
      "\t\tdont know\t152\n",
      "\t\tdont think\t147\n",
      "\t\tfeel like\t100\n",
      "\t\tdont want\t70\n",
      "\t\twould say\t69\n",
      "\tMost common 3-grams:\n",
      "\t\tim pretty sure\t19\n",
      "\t\tfeel like im\t11\n",
      "\t\tget along well\t8\n",
      "\t\tanything may affect\t8\n",
      "\t\tmay affect way\t8\n",
      "ENFJ\n",
      "\tMost common 1-grams:\n",
      "\t\tim\t1672\n",
      "\t\tlike\t1596\n",
      "\t\tthink\t1201\n",
      "\t\tpeople\t1090\n",
      "\t\tdont\t1069\n",
      "\tMost common 2-grams:\n",
      "\t\tfeel like\t149\n",
      "\t\tdont know\t149\n",
      "\t\tdont think\t114\n",
      "\t\tim sure\t79\n",
      "\t\tsounds like\t64\n",
      "\tMost common 3-grams:\n",
      "\t\tsent iphone using\t29\n",
      "\t\tfeel like im\t17\n",
      "\t\t！ ？ ！\t14\n",
      "\t\t？ ！ ？\t14\n",
      "\t\tim pretty sure\t11\n",
      "INFP\n",
      "\tMost common 1-grams:\n",
      "\t\tlike\t15290\n",
      "\t\tim\t15012\n",
      "\t\tthink\t11036\n",
      "\t\tdont\t10853\n",
      "\t\tpeople\t9673\n",
      "\tMost common 2-grams:\n",
      "\t\tdont know\t1718\n",
      "\t\tfeel like\t1582\n",
      "\t\tdont think\t1180\n",
      "\t\tim sure\t642\n",
      "\t\tdont like\t608\n",
      "\tMost common 3-grams:\n",
      "\t\tfeel like im\t177\n",
      "\t\tim pretty sure\t145\n",
      "\t\tsent iphone using\t108\n",
      "\t\tknow youre infp\t98\n",
      "\t\tdont really know\t90\n",
      "ENFP\n",
      "\tMost common 1-grams:\n",
      "\t\tlike\t5823\n",
      "\t\tim\t5733\n",
      "\t\tthink\t3924\n",
      "\t\tdont\t3752\n",
      "\t\tpeople\t3434\n",
      "\tMost common 2-grams:\n",
      "\t\tdont know\t588\n",
      "\t\tfeel like\t547\n",
      "\t\tdont think\t402\n",
      "\t\tim sure\t244\n",
      "\t\tsounds like\t206\n",
      "\tMost common 3-grams:\n",
      "\t\tsent iphone using\t72\n",
      "\t\tim pretty sure\t58\n",
      "\t\tknow youre enfp\t38\n",
      "\t\tdont feel like\t36\n",
      "\t\tfeel like im\t36\n",
      "ISFP\n",
      "\tMost common 1-grams:\n",
      "\t\tlike\t2275\n",
      "\t\tim\t2260\n",
      "\t\tdont\t1574\n",
      "\t\tthink\t1510\n",
      "\t\treally\t1263\n",
      "\tMost common 2-grams:\n",
      "\t\tdont know\t236\n",
      "\t\tfeel like\t179\n",
      "\t\tdont think\t158\n",
      "\t\tdont like\t111\n",
      "\t\tim sure\t97\n",
      "\tMost common 3-grams:\n",
      "\t\tsent smg 360v\t26\n",
      "\t\tsmg 360v using\t26\n",
      "\t\tfeel like im\t24\n",
      "\t\tlove love love\t18\n",
      "\t\tim pretty sure\t17\n",
      "ISTP\n",
      "\tMost common 1-grams:\n",
      "\t\tim\t2442\n",
      "\t\tlike\t2417\n",
      "\t\tdont\t2209\n",
      "\t\tthink\t1638\n",
      "\t\tpeople\t1542\n",
      "\tMost common 2-grams:\n",
      "\t\tdont know\t285\n",
      "\t\tdont think\t175\n",
      "\t\tfeel like\t138\n",
      "\t\tdont like\t136\n",
      "\t\tpretty much\t111\n",
      "\tMost common 3-grams:\n",
      "\t\tsent iphone using\t39\n",
      "\t\tanything may affect\t18\n",
      "\t\tmay affect way\t18\n",
      "\t\taffect way answer\t18\n",
      "\t\tim pretty sure\t18\n",
      "ISFJ\n",
      "\tMost common 1-grams:\n",
      "\t\tim\t1543\n",
      "\t\tlike\t1349\n",
      "\t\tthink\t1034\n",
      "\t\tdont\t1012\n",
      "\t\tpeople\t814\n",
      "\tMost common 2-grams:\n",
      "\t\tdont know\t149\n",
      "\t\tfeel like\t136\n",
      "\t\tdont think\t93\n",
      "\t\tim sure\t81\n",
      "\t\tdont like\t72\n",
      "\tMost common 3-grams:\n",
      "\t\tsent iphone using\t34\n",
      "\t\tforumwelcome forumwelcome forumwelcome\t17\n",
      "\t\tfeel like im\t16\n",
      "\t\tim pretty sure\t16\n",
      "\t\tmi mi mi\t14\n",
      "ISTJ\n",
      "\tMost common 1-grams:\n",
      "\t\tlike\t1613\n",
      "\t\tim\t1507\n",
      "\t\tdont\t1305\n",
      "\t\tthink\t989\n",
      "\t\tpeople\t871\n",
      "\tMost common 2-grams:\n",
      "\t\tdont know\t182\n",
      "\t\tdont think\t117\n",
      "\t\tdont like\t113\n",
      "\t\tfeel like\t85\n",
      "\t\tim sure\t81\n",
      "\tMost common 3-grams:\n",
      "\t\textraverted intuition ne\t17\n",
      "\t\tintroverted thinking ti\t16\n",
      "\t\tfeel like im\t15\n",
      "\t\tim pretty sure\t14\n",
      "\t\twelcome forum enjoy\t13\n",
      "ESTP\n",
      "\tMost common 1-grams:\n",
      "\t\tlike\t721\n",
      "\t\tim\t608\n",
      "\t\tdont\t559\n",
      "\t\tthink\t487\n",
      "\t\tpeople\t427\n",
      "\tMost common 2-grams:\n",
      "\t\tdont know\t73\n",
      "\t\tdont think\t70\n",
      "\t\tfeel like\t44\n",
      "\t\tdont like\t41\n",
      "\t\tim sure\t35\n",
      "\tMost common 3-grams:\n",
      "\t\tsent iphone using\t23\n",
      "\t\testp im looking\t15\n",
      "\t\tim looking interview\t15\n",
      "\t\tlooking interview really\t15\n",
      "\t\tinterview really want\t15\n",
      "ESFP\n",
      "\tMost common 1-grams:\n",
      "\t\tlike\t340\n",
      "\t\tim\t337\n",
      "\t\tdont\t273\n",
      "\t\tthink\t253\n",
      "\t\tpeople\t210\n",
      "\tMost common 2-grams:\n",
      "\t\tdont know\t49\n",
      "\t\tdont think\t30\n",
      "\t\tfeel like\t27\n",
      "\t\tsounds like\t17\n",
      "\t\tsent iphone\t17\n",
      "\tMost common 3-grams:\n",
      "\t\tsent iphone 6\t12\n",
      "\t\tiphone 6 plus\t12\n",
      "\t\t6 plus using\t12\n",
      "\t\tsent gti 9000\t10\n",
      "\t\tgti 9000 using\t10\n",
      "ESTJ\n",
      "\tMost common 1-grams:\n",
      "\t\tim\t287\n",
      "\t\tlike\t262\n",
      "\t\tthink\t233\n",
      "\t\tdont\t228\n",
      "\t\tpeople\t206\n",
      "\tMost common 2-grams:\n",
      "\t\tdont know\t28\n",
      "\t\tdont think\t26\n",
      "\t\tim sure\t21\n",
      "\t\tbest friend\t16\n",
      "\t\tim going\t16\n",
      "\tMost common 3-grams:\n",
      "\t\tive never met\t4\n",
      "\t\tmake extra cash\t4\n",
      "\t\tpeople never met\t4\n",
      "\t\thi internally talking\t4\n",
      "\t\tim pretty sure\t3\n",
      "ESFJ\n",
      "\tMost common 1-grams:\n",
      "\t\tlike\t379\n",
      "\t\tim\t362\n",
      "\t\tthink\t309\n",
      "\t\tdont\t265\n",
      "\t\tpeople\t249\n",
      "\tMost common 2-grams:\n",
      "\t\tdont know\t42\n",
      "\t\tfeel like\t37\n",
      "\t\tdont think\t32\n",
      "\t\tsent iphone\t29\n",
      "\t\tiphone using\t27\n",
      "\tMost common 3-grams:\n",
      "\t\tsent iphone using\t27\n",
      "\t\tiphone using tapatalkthe\t9\n",
      "\t\tmyers briggs mbti\t6\n",
      "\t\tmbti cat edition\t5\n",
      "\t\tcat edition part\t5\n"
     ]
    }
   ],
   "source": [
    "for type in types:\n",
    "    print(type)\n",
    "    for n in ns:\n",
    "        print(\"\\tMost common %d-grams:\" % n)\n",
    "        for entry, count in n_gram_freqs_by_type[type][n].most_common(5):\n",
    "            print(\"\\t\\t\" + \" \".join(entry) + \"\\t\" + str(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mbti-nlp",
   "language": "python",
   "name": "mbti-nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
